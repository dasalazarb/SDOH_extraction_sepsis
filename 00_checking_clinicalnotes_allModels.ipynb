{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f183f5-c4b1-4595-aae8-74bfff6ab09a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# !pip install transformers accelerate torch sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96d8a02-1fe1-4af3-a5a6-5eacf242f37b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# !pip install psycopg2 transformers accelerate torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28de69f-fdbe-463a-949a-8d06c9680fa3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# note = ul.get_clinical_note(subject_id=10000032)\n",
    "# prompt = ul.sdh_prompt(note)\n",
    "# response = pipe(prompt, max_new_tokens=400)[0]['generated_text']\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62d4982-0d78-461b-a67d-17f56cf9983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import utils_llm as ul\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, T5ForConditionalGeneration, MegatronBertForCausalLM, AutoModelForSeq2SeqLM, LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b0b938-6bc0-4133-a970-045135c2e7bb",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df09e32-99ef-4461-bf72-b650705dc907",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Mistral-7B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d633cd-ab4a-45f5-843d-f395c152af6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'I:/Mistral-7B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, pad_token_id=tokenizer.eos_token_id, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c9ff85-ec54-4f53-8000-da26d021f267",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = ul.get_notes_for_first_n_notes(1)\n",
    "prompt = ul.sdh_single_prompt(notes[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da6a174-b411-49f8-baf0-184cf96f8f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pipe(prompt, max_new_tokens=400)[0]['generated_text']\n",
    "\n",
    "if prompt in output:\n",
    "    print(output.replace(prompt, '').strip())\n",
    "else:\n",
    "    print(output.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7c6c9b-74e0-4daf-858c-0ea1494e9aca",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25d7fc3-0175-4481-a54e-17087fc08504",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# BioMistral-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c811f7-a3fe-4d08-87ad-3a5fe8dae287",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'I:/BioMistral-7B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             device_map=\"auto\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=50000, \n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_k=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4630eb44-365c-46f5-9f14-ed7e0e63d159",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = ul.get_notes_for_first_n_notes(1)\n",
    "prompt = ul.sdh_prompt(notes[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65464015-0f5f-4451-9aba-6be721119f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_input = input(\"You: \")\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": user_input}\n",
    "# ]\n",
    "\n",
    "# prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# response = pipe(prompt)\n",
    "\n",
    "# generated_text = response[0]['generated_text']\n",
    "\n",
    "# if prompt in generated_text:\n",
    "#     llm_reply = generated_text.replace(prompt, \"\").strip()\n",
    "# else:\n",
    "#     llm_reply = generated_text.strip()\n",
    "\n",
    "# print(f\"LLM: {llm_reply}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3145c598-5ca8-4c5f-80d5-6364bc8b7c84",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f153e28f-fa4b-413c-90e4-75f0f7ae8ac2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# clinicalt5-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d2a670-1027-4ea7-81d8-8be869776f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'I:/Clinical-T5-Large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_id)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "pipe = pipeline('text2text-generation', model=model, tokenizer=tokenizer, pad_token_id=tokenizer.eos_token_id, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11251763-20e6-4ac2-9635-a64ceb2b00c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = ul.get_notes_for_first_n_notes(1)\n",
    "prompt = ul.sdh_single_prompt(notes[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a021dd-dc3e-48b4-ad19-e8fbf1b9a5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'What drug is used in the following text: He used omeprazol.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23f9031-3d3a-445b-a5bd-07fd340704de",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pipe(prompt, max_new_tokens=400)[0]['generated_text']\n",
    "\n",
    "if prompt in output:\n",
    "    print(output.replace(prompt, '').strip())\n",
    "else:\n",
    "    print(output.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08a3bc2-ec73-4140-b6fd-af19bdb4d9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "note = \"38 y/o F, single mother of 2 (4 y/o and 6 y/o), h/o HTN and anxiety presents with medication nonadherence due to unstable PT barista job (~20 h/wk) and unreliable public transport causing missed appts and work shifts. Reports 2 mo rent arrears, late-payment notice pending eviction. No personal vehicle, bus cuts limit mobility. Ex-spouse provides no support; sister OOS; one friend for occasional childcare. Limited social support. Plan: continue lisinopril 10 mg daily, add SSRI; refer to housing assistance, workforce development, bus-pass voucher, subsidized childcare, social work, and food pantry.\"\n",
    "prompt = f\"Answer step by step: \\\n",
    "1. Identify medication.\\\n",
    "2. Evaluate context.\\\n",
    "3. Output medications. Note: {note}\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=150,\n",
    "    num_beams=4,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ")\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcc266d-3fa9-4215-a87a-ae6389e1d3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_id)\n",
    "\n",
    "prompt = 'Q: Can methotrexate be combined with an antibiotic?'\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=600,\n",
    "    length_penalty=1.6,\n",
    "    num_beams=4,\n",
    "    no_repeat_ngram_size=3,\n",
    "    temperature=0.8,\n",
    "    top_k=150,\n",
    "    top_p=0.92,\n",
    "    repetition_penalty=2.1,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2333b7-e84f-4cb6-814d-aea7b89d87da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "# output_ids = model.generate(input_ids, max_length=50, num_beams=4, early_stopping=True)\n",
    "# generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "# print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2788a036-4ccc-4e55-bc38-2af67f6dcb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_id = 'I:/modernBERT'\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb9a1a7-389a-42a3-9fca-98b0dc5fedf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'The patient is healthy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551232cd-fec0-4323-95dc-e7bc0c67ef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5a49b1-a327-49ae-a69a-cb07529fd320",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    task=\"text-classification\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eab667-8121-405e-8b32-15a343441daa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a5d321-54ff-469b-8ba3-15e94c377f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(\"The patient is healthy and happy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed8bc6-eb9d-43fc-b896-6791bb570034",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581c1150-1aa6-416d-8a53-0b079fbc37f1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# gatortronS (gatortronGPT -> decoder-only (GPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1b4b2b-4b2e-46bd-aedc-57242ebb89f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_id = 'I:/gatortronS'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = MegatronBertForCausalLM.from_pretrained(model_id, is_decoder=True)#.to('cuda:0')\n",
    "assert tokenizer.vocab_size <= model.config.vocab_size\n",
    "# pipe = pipeline('text-generation', model=model, tokenizer=tokenizer, pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4da75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_and_hadm_ids = pd.read_csv('C:/Users/salazarda/Downloads/SDOH_MIMICIII_physio_release.csv')\n",
    "subject_and_hadm_ids = list(subject_and_hadm_ids.loc[:, ['patient_id', 'note_id']].drop_duplicates().itertuples(index=False, name=None))\n",
    "\n",
    "notes = ul.get_clinical_notes_mimic3(subject_and_hadm_ids)\n",
    "notes = notes[0:5]\n",
    "\n",
    "sdoh_output = []\n",
    "\n",
    "for note in notes:\n",
    "    meta = {\n",
    "        'subject_id': note[0],\n",
    "        'hadm_id': note[1],\n",
    "        'row_id': note[2],\n",
    "        'charttime': note[3].isoformat() if note[3] else None\n",
    "    }\n",
    "\n",
    "    outputs_per_note = meta.copy()  # Start with metadata\n",
    "\n",
    "    for sdoh in tqdm(['Employment status', 'Housing issues', 'Transportation issues', 'Parental status', 'Relationship status', 'Social support']):\n",
    "        instruction = {\n",
    "            'Employment status': 'Employment status: Whether the patient is currently employed, unemployed, underemployed, disability, retired, student, or unknown. LABELS: [employed, unemployed, underemployed, disability, retired, student, unknown]',\n",
    "            'Housing issues': 'Housing issues: Any mention of financial status, undomiciled, other. LABELS: [financial status, undomiciled, other, unknown]', \n",
    "            'Transportation issues': 'Transportation issues: Any reference to transportation difficulties such as distance, resources, other. LABELS: [distance, resources, other, unknown]', \n",
    "            'Parental status':'Parental status: Whether the patient has a child under 18 years old. LABELS: [yes, no, unknown]',\n",
    "            'Relationship status': 'Relationship status: Whether the patient is widowed, divorced, single. LABELS: [married, partnered, widowed, divorced, single, unknown]',\n",
    "            'Social support': 'Social support: It does include informal or emotional support from family members, friends, or romantic partners unless such support is clearly mediated through a formal care plan by a social worker or case manager. LABELS: [presence, absence, unknown]'\n",
    "        }\n",
    "        \n",
    "        prompt = ul.sdh_single_prompt(note[4], sdoh, instruction[sdoh])\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        inputs['position_ids'] = torch.arange(0, inputs['input_ids'].size(1), dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=600,\n",
    "                length_penalty=1.6,\n",
    "                num_beams=10,\n",
    "                no_repeat_ngram_size=3,\n",
    "                temperature=0.8,\n",
    "                do_sample=True,\n",
    "                top_k=15,\n",
    "                top_p=0.95,\n",
    "                repetition_penalty=2.1,\n",
    "                early_stopping=True\n",
    "            )\n",
    "\n",
    "        output_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        outputs_per_note[sdoh] = output_text\n",
    "    \n",
    "    sdoh_output.append(outputs_per_note)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591523bc-cbaa-4843-9b15-92438a2f4a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = ul.get_notes_for_first_n_notes(1)\n",
    "prompt = ul.sdh_prompt_guevara(notes[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab07d73c-1cb2-4447-b960-d84f70285e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ec7940-e9d3-4f26-b50b-eee0dfab9a11",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output_ids = model.generate(**inputs, max_new_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51084c94-42a4-4e79-b4d8-edede4bee8ba",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pipe(prompt, max_new_tokens=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa063cd-bb3d-4b67-bda8-6bfc52f4b39d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = pipe(prompt, max_new_tokens=400)[0]['generated_text']\n",
    "\n",
    "if prompt in output:\n",
    "    print(output.replace(prompt, '').strip())\n",
    "else:\n",
    "    print(output.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d687f2-3e19-4e25-95c9-bff937ed098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The patient presents with abdominal pain and\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_length=200, do_sample=True, temperature=0.7)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc18a0f8-3b6d-4bd8-b1a1-f9e0e19f7430",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b463cce-6604-4a59-b190-ec6f6c055fe7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# meditron-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd86e7f0-385d-4a0c-a7cc-b617631219ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'I:/meditron-7b'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "pipe = pipeline('text-generation', model=model, tokenizer=tokenizer, pad_token_id=tokenizer.eos_token_id, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50139818-e1da-487a-801a-1bf690a8673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = ul.get_notes_for_first_n_notes(1)\n",
    "prompt = ul.sdh_single_prompt(notes[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34db8703-ab7c-497b-96f7-563b485d3385",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = pipe(prompt, max_new_tokens=400)\n",
    "\n",
    "if prompt in output:\n",
    "    print(output[0]['generated_text'].strip().replace(prompt, ''))\n",
    "else:\n",
    "    print(output[0]['generated_text'].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213852c3-1b2d-4c0a-9902-6376eee857a0",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cbc0c5-b128-4f0d-89d4-4bcc41991da6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# meditron3-8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e548af-1cac-46bd-b95f-2ea3af57444e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'I:/meditron3-8b'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "pipe = pipeline('text-generation', model=model, tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22362668-e5a9-4ee6-9d22-5a99b5814a2e",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b3dbde-2899-4763-86bd-72613a467d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = ul.get_notes_for_first_n_notes(1)\n",
    "prompt = ul.sdh_prompt(notes[0][3])\n",
    "# pipe(prompts[0], max_new_tokens=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae91a139-109e-4593-ae54-7bd1d978243a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# LlamaCare + MIMIC III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dda189-0411-4185-acf7-f14b613e2a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'I:/LlamaCare'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = LlamaForCausalLM.from_pretrained(model_id)\n",
    "pipe = pipeline('text-generation', model=model, tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb16413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "base_model = \"I:/Llama-2-13b-hf\"          # Llama-2-13b-hf\n",
    "adapter_id = \"I:/LlamaCare\"             # LoRA de LlamaCare\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                bnb_4bit_quant_type=\"nf4\",\n",
    "                                bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(base_model, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model,\n",
    "                                             quantization_config=bnb_config,\n",
    "                                             device_map=\"auto\")\n",
    "model = PeftModel.from_pretrained(model, adapter_id)  # aplicar el LoRA\n",
    "\n",
    "def chat(prompt, max_new_tokens=256):\n",
    "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    out = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    return tok.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "print(chat(\"Eres un asistente médico. Explica el manejo inicial de DM2.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b89ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "subject_and_hadm_ids = pd.read_csv('C:/Users/salazarda/Downloads/SDOH_MIMICIII_physio_release.csv')\n",
    "subject_and_hadm_ids = list(subject_and_hadm_ids.loc[:, ['patient_id', 'note_id']].drop_duplicates().itertuples(index=False, name=None))\n",
    "\n",
    "notes = ul.get_clinical_notes_mimic3(subject_and_hadm_ids)\n",
    "notes = random.sample(notes, 5)\n",
    "# notes = notes[0:50]\n",
    "\n",
    "sdoh_output = []\n",
    "\n",
    "for note in notes:\n",
    "    meta = {\n",
    "        'subject_id': note[0],\n",
    "        'hadm_id': note[1],\n",
    "        'row_id': note[2],\n",
    "        'charttime': note[3].isoformat() if note[3] else None\n",
    "    }\n",
    "\n",
    "    outputs_per_note = meta.copy()  # Start with metadata\n",
    "\n",
    "    for sdoh in tqdm(['Employment status', 'Housing issues', 'Transportation issues', 'Parental status', 'Relationship status', 'Social support']):\n",
    "        instruction = {\n",
    "            'Employment status': 'Employment status: Whether the patient is currently employed, unemployed, underemployed, disability, retired, student, or unknown. LABELS: [employed, unemployed, underemployed, disability, retired, student, unknown]',\n",
    "            'Housing issues': 'Housing issues: Any mention of financial status, undomiciled, other. LABELS: [financial status, undomiciled, other, unknown]', \n",
    "            'Transportation issues': 'Transportation issues: Any reference to transportation difficulties such as distance, resources, other. LABELS: [distance, resources, other, unknown]', \n",
    "            'Parental status':'Parental status: Whether the patient has a child under 18 years old. LABELS: [yes, no, unknown]',\n",
    "            'Relationship status': 'Relationship status: Whether the patient is widowed, divorced, single. LABELS: [married, partnered, widowed, divorced, single, unknown]',\n",
    "            'Social support': 'Social support: It does include informal or emotional support from family members, friends, or romantic partners unless such support is clearly mediated through a formal care plan by a social worker or case manager. LABELS: [presence, absence, unknown]'\n",
    "        }\n",
    "\n",
    "        prompt = ul.sdh_single_prompt(note[4], sdoh, instruction[sdoh])\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=600,\n",
    "                length_penalty=1.6,\n",
    "                num_beams=10,\n",
    "                no_repeat_ngram_size=3,\n",
    "                temperature=0.8,\n",
    "                do_sample=True,\n",
    "                top_k=15,\n",
    "                top_p=0.95,\n",
    "                repetition_penalty=2.1,\n",
    "                early_stopping=True\n",
    "            )\n",
    "\n",
    "        output_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        outputs_per_note[sdoh] = output_text\n",
    "\n",
    "    sdoh_output.append(outputs_per_note)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa200d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "time_start = datetime.now()\n",
    "timestamp = time_start.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "subject_and_hadm_ids = pd.read_csv('C:/Users/salazarda/Downloads/SDOH_MIMICIII_physio_release.csv')\n",
    "subject_and_hadm_ids = list(subject_and_hadm_ids.loc[:,['patient_id', 'note_id']].drop_duplicates().itertuples(index=False, name=None))\n",
    "notes = ul.get_clinical_notes_mimic3(subject_and_hadm_ids)\n",
    "notes = random.sample(notes, 2)\n",
    "# notes = notes[0:50]\n",
    "\n",
    "prompts = []\n",
    "metadata = []\n",
    "\n",
    "for subject_id, hadm_id, row_id, charttime, note_text in notes:\n",
    "    prompts.append(ul.sdh_prompt_guevara_v2(note_text))\n",
    "    metadata.append({\n",
    "        \"subject_id\": subject_id,\n",
    "        \"hadm_id\": hadm_id,\n",
    "        \"row_id\": row_id,\n",
    "        \"charttime\": charttime.isoformat() if charttime else None\n",
    "    })\n",
    "\n",
    "batch_size = 16\n",
    "parsed_list = []\n",
    "final_outputs = []\n",
    "\n",
    "for i in tqdm(range(0, len(prompts), batch_size)):\n",
    "    batch_prompts = prompts[i:i+batch_size]\n",
    "    batch_meta = metadata[i:i+batch_size]\n",
    "    batch_responses = pipe(batch_prompts, max_new_tokens=400)\n",
    "    \n",
    "    for meta, raw, prompt in zip(batch_meta, batch_responses, batch_prompts):\n",
    "        text = raw[0]['generated_text']\n",
    "        if prompt in text:\n",
    "            text = text.replace(prompt, \"\").strip()\n",
    "        text = {'text': text}\n",
    "    \n",
    "        final_outputs.append({**meta, **text})\n",
    "\n",
    "ul.save_to_jsonl(final_outputs, model_id, timestamp)\n",
    "\n",
    "n = len(list(set([i['subject_id'] for i in final_outputs])))\n",
    "\n",
    "print(f' ... For {n} patients and {len(notes)} notes, it took {datetime.now() - time_start} ... ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55c5749-6a32-477f-840b-0e5884259750",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = ul.get_notes_for_first_n_notes(1)\n",
    "prompt = ul.sdh_single_prompt(notes[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0564e1a9-2a05-444b-9214-b349235ba768",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pipe(prompt, max_new_tokens=400)\n",
    "\n",
    "if prompt in output:\n",
    "    print(output.replace(prompt, '').strip())\n",
    "else:\n",
    "    print(output.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbed8ff-80a3-4056-8980-720561e79669",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Qwen1.5-0.5B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d54c67-c14d-48c7-9d0c-267dd2279668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import utils_llm as ul\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eccc294-0fcc-4634-89cf-355fd84bb757",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'I:/Qwen1.5-0.5B-LoRA-bioinstruct'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "pipe = pipeline('text-generation', model=model, tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4bb953-03f8-4b6c-b94f-48981eacbe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = ul.get_notes_for_first_n_notes(1)\n",
    "prompt = ul.sdh_single_prompt(notes[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca94a151-52fe-4193-8733-fc48092e2446",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529fed66-6fdd-4ff7-9b60-748b43723727",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = pipe(prompt, max_new_tokens=400)[0]['generated_text']\n",
    "\n",
    "if prompt in output:\n",
    "    print(output.replace(prompt, '').strip())\n",
    "else:\n",
    "    print(output.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9914021-a0c9-4ea3-8389-74bf8c59446b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# AWS Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314a6754-a396-4e69-b117-c0f4a8850b84",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed950811-f97b-471a-8881-5980abaab38b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svc = boto3.client(\"bedrock\", region_name=\"us-east-1\")\n",
    "\n",
    "# list_foundation_models\n",
    "resp = svc.list_foundation_models()\n",
    "print(\"Foundation models:\")\n",
    "for mdl in resp.get(\"modelSummaries\", []):\n",
    "    if 'TEXT' in mdl['outputModalities']:\n",
    "        print(\" •\", mdl[\"modelId\"])\n",
    "    else:\n",
    "        pass\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662b88fe-81e4-4b0b-956a-581dc06b63f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import utils_llm as ul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a269474-1aa2-4da8-b155-9392a09e3f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "    notes = ul.get_notes_for_first_n_notes(1)\n",
    "    prompt = ul.sdh_prompt(notes[0][3])\n",
    "    \n",
    "    # prompt = 'Extract information for: A 62-year-old male school bus driver with a history of hypertension (diagnosed 2015) and hyperlipidemia (diagnosed 2018) presents with a two-day history of intermittent substernal, pressure-like chest pain radiating to the left arm, rated 6/10 in intensity. The discomfort occurs both at rest and with minimal exertion, lasting five to ten minutes each episode, and is accompanied by mild dyspnea on exertion but no diaphoresis or syncope. He denies tobacco, alcohol, or illicit drug use. He lives with his spouse in a two-bedroom apartment and commutes via public transportation. On exam, his blood pressure is 150/90 mmHg, heart rate 88 bpm, respiratory rate 18 breaths/min, SpO₂ 98% on room air, and temperature 36.8 °C. He appears in mild distress but is alert; cardiovascular exam shows a regular rate and rhythm without murmurs or gallops, lungs are clear bilaterally, abdomen is soft and non-tender, and neurologic exam is non-focal. His medications include lisinopril 20 mg daily and atorvastatin 40 mg nightly. The plan is to rule out acute coronary syndrome with ECG and serial troponins, administer aspirin 325 mg PO now and nitroglycerin 0.4 mg SL PRN, continue home medications, monitor vitals and pain every four hours, and consult cardiology for further evaluation.'  \n",
    "\n",
    "    list_titan_models = ['amazon.titan-tg1-large',\n",
    "    'amazon.nova-premier-v1:0:8k',\n",
    "    'amazon.nova-premier-v1:0:20k',\n",
    "    'amazon.nova-premier-v1:0:1000k',\n",
    "    'amazon.nova-premier-v1:0:mm',\n",
    "    'amazon.nova-premier-v1:0',\n",
    "    'amazon.titan-text-premier-v1:0']\n",
    "    \n",
    "    payload = {\n",
    "        \"inputText\": prompt,\n",
    "        \"textGenerationConfig\": {\n",
    "            \"maxTokenCount\": 512,\n",
    "            \"temperature\": 0.7\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # payload = {\n",
    "    #     \"prompt\": \"\\n\\nHuman: \" + prompt + \"\\n\\nAssistant:\",\n",
    "    #     'max_tokens_to_sample': 4000\n",
    "    # }\n",
    "\n",
    "    # messages = [\n",
    "    #     {\"role\": \"system\",    \"content\": \"You are a clinical NLP assistant.\"},\n",
    "    #     {\"role\": \"assistant\", \"content\": \"Ready to extract Employment status_, Housing issues, Transportation needs, Parental status, Relationship status, Social support, and Substance Use from a clinical note.\"},\n",
    "    #     {\"role\": \"user\",      \"content\": prompt}\n",
    "    # ]\n",
    "\n",
    "    # payload = {\n",
    "    #     \"messages\": messages,\n",
    "    #     \"temperature\": 0.7,\n",
    "    #     \"max_token_count\": 256,\n",
    "    #     \"top_p\": 0.5\n",
    "    # }\n",
    "    \n",
    "    response = client.invoke_model(\n",
    "        modelId=\"amazon.titan-text-premier-v1:0\",\n",
    "        contentType=\"application/json\",\n",
    "        accept=\"application/json\",\n",
    "        body=json.dumps(payload).encode(\"utf-8\")\n",
    "    )\n",
    "    \n",
    "    raw = response[\"body\"].read().decode(\"utf-8\")\n",
    "    try:\n",
    "        out = json.loads(raw)\n",
    "        print(json.dumps(out, indent=2, ensure_ascii=False))\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Raw:\", raw)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75c2dfb-c13c-43ed-9dbd-6ee456d6b019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3183398-72ca-471e-ba2e-3b1a1ebda9b7",
   "metadata": {},
   "source": [
    "# Flan-T5 XL + MIMIC III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c7e680-a7fe-4e75-b49d-03bf42022d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'I:/Flan-t5-xl'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448a1d41-0321-47b5-8141-6590528f7bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_and_hadm_ids = pd.read_csv('C:/Users/salazarda/Downloads/SDOH_MIMICIII_physio_release.csv')\n",
    "subject_and_hadm_ids = list(subject_and_hadm_ids.loc[:, ['patient_id', 'note_id']].drop_duplicates().itertuples(index=False, name=None))\n",
    "\n",
    "notes = ul.get_clinical_notes_mimic3(subject_and_hadm_ids)\n",
    "notes = notes[0:5]\n",
    "\n",
    "sdoh_output = []\n",
    "\n",
    "for note in notes:\n",
    "    meta = {\n",
    "        'subject_id': note[0],\n",
    "        'hadm_id': note[1],\n",
    "        'row_id': note[2],\n",
    "        'charttime': note[3].isoformat() if note[3] else None\n",
    "    }\n",
    "\n",
    "    outputs_per_note = meta.copy()  # Start with metadata\n",
    "\n",
    "    for sdoh in tqdm(['Employment status', 'Housing issues', 'Transportation issues', 'Parental status', 'Relationship status', 'Social support']):\n",
    "        instruction = {\n",
    "            'Employment status': 'Employment status: Whether the patient is currently employed, unemployed, underemployed, disability, retired, student, or unknown. LABELS: [employed, unemployed, underemployed, disability, retired, student, unknown]',\n",
    "            'Housing issues': 'Housing issues: Any mention of financial status, undomiciled, other. LABELS: [financial status, undomiciled, other, unknown]', \n",
    "            'Transportation issues': 'Transportation issues: Any reference to transportation difficulties such as distance, resources, other. LABELS: [distance, resources, other, unknown]', \n",
    "            'Parental status':'Parental status: Whether the patient has a child under 18 years old. LABELS: [yes, no, unknown]',\n",
    "            'Relationship status': 'Relationship status: Whether the patient is widowed, divorced, single. LABELS: [married, partnered, widowed, divorced, single, unknown]',\n",
    "            'Social support': 'Social support: It does include informal or emotional support from family members, friends, or romantic partners unless such support is clearly mediated through a formal care plan by a social worker or case manager. LABELS: [presence, absence, unknown]'\n",
    "        }\n",
    "\n",
    "        prompt = ul.sdh_single_prompt(note[4], sdoh, instruction[sdoh])\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=600,\n",
    "                length_penalty=1.6,\n",
    "                num_beams=10,\n",
    "                no_repeat_ngram_size=3,\n",
    "                temperature=0.8,\n",
    "                do_sample=True,\n",
    "                top_k=15,\n",
    "                top_p=0.95,\n",
    "                repetition_penalty=2.1,\n",
    "                early_stopping=True\n",
    "            )\n",
    "\n",
    "        output_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        outputs_per_note[sdoh] = output_text\n",
    "\n",
    "    sdoh_output.append(outputs_per_note)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4682c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdoh_output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dab085",
   "metadata": {},
   "source": [
    "# ModernBERT + MIMIC III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ef41c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "model_id = \"I:/modernBERT-Large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_id)\n",
    "\n",
    "text = \"The capital of France is [MASK].\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# To get predictions for the mask:\n",
    "masked_index = inputs[\"input_ids\"][0].tolist().index(tokenizer.mask_token_id)\n",
    "predicted_token_id = outputs.logits[0, masked_index].argmax(axis=-1)\n",
    "predicted_token = tokenizer.decode(predicted_token_id)\n",
    "print(\"Predicted token:\", predicted_token)\n",
    "# Predicted token:  Paris\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be50166-ef29-459e-b17a-3eef64cc2e50",
   "metadata": {},
   "source": [
    "# Mistral-7B-Instruct + MIMIC III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de56ec1b-a2a8-40df-a4a4-b4c295d8928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'I:/Mistral-7B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, pad_token_id=tokenizer.eos_token_id, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c156c9-c5e4-4785-a11a-a8577fc454d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "time_start = datetime.now()\n",
    "timestamp = time_start.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "subject_and_hadm_ids = pd.read_csv('C:/Users/salazarda/Downloads/SDOH_MIMICIII_physio_release.csv')\n",
    "subject_and_hadm_ids = list(subject_and_hadm_ids.loc[:,['patient_id', 'note_id']].drop_duplicates().itertuples(index=False, name=None))\n",
    "notes = ul.get_clinical_notes_mimic3(subject_and_hadm_ids)\n",
    "# notes = random.sample(notes, 10)\n",
    "# notes = notes[0:50]\n",
    "\n",
    "prompts = []\n",
    "metadata = []\n",
    "\n",
    "for subject_id, hadm_id, row_id, charttime, note_text in notes:\n",
    "    prompts.append(ul.sdh_prompt_guevara_v2(note_text))\n",
    "    metadata.append({\n",
    "        \"subject_id\": subject_id,\n",
    "        \"hadm_id\": hadm_id,\n",
    "        \"row_id\": row_id,\n",
    "        \"charttime\": charttime.isoformat() if charttime else None\n",
    "    })\n",
    "\n",
    "batch_size = 16\n",
    "parsed_list = []\n",
    "final_outputs = []\n",
    "\n",
    "for i in tqdm(range(0, len(prompts), batch_size)):\n",
    "    batch_prompts = prompts[i:i+batch_size]\n",
    "    batch_meta = metadata[i:i+batch_size]\n",
    "    batch_responses = pipe(batch_prompts, max_new_tokens=400)\n",
    "    \n",
    "    for meta, raw, prompt in zip(batch_meta, batch_responses, batch_prompts):\n",
    "        text = raw[0]['generated_text']\n",
    "        if prompt in text:\n",
    "            text = text.replace(prompt, \"\").strip()\n",
    "        text = {'text': text}\n",
    "    \n",
    "        final_outputs.append({**meta, **text})\n",
    "\n",
    "ul.save_to_jsonl(final_outputs, model_id, timestamp)\n",
    "\n",
    "n = len(list(set([i['subject_id'] for i in final_outputs])))\n",
    "\n",
    "print(f' ... For {n} patients and {len(notes)} notes, it took {datetime.now() - time_start} ... ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4842b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee1496a-03d5-411f-a7aa-691766bae797",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd0fa4d-9d8a-4fb4-974b-9d2338fdd0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "records = []\n",
    "\n",
    "pattern = re.compile(r'\"(?:Employment status|Housing issues|Transportation issues|Parental status|Relationship status|Social support)\"\\s*:\\s*\"[^\"]+\"')\n",
    "\n",
    "for entry in final_outputs:\n",
    "    base = {\"subject_id\": entry[\"subject_id\"], \"hadm_id\": entry[\"hadm_id\"], \"charttime\": entry[\"charttime\"], \"row_id\": entry[\"row_id\"]}\n",
    "    matches = pattern.findall(entry[\"text\"])\n",
    "    sdhs = {label.split(':')[0].replace('\"',''): label.split(':')[1].replace('\"','') for label in matches}\n",
    "    base.update(sdhs)\n",
    "    records.append(base)\n",
    "    \n",
    "df = pd.DataFrame(records)\n",
    "df = df.map(lambda x: x.lower().strip() if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ef6304-be37-4a93-815c-6e359385a960",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8a113a-e83e-4a85-a82b-02a60b4ae8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# records = []\n",
    "\n",
    "# pattern = re.compile(r'SDH_([^:]+): \\[([^\\]]+)\\]')\n",
    "\n",
    "# for entry in final_outputs:\n",
    "#     base = {\"subject_id\": entry[\"subject_id\"], \"hadm_id\": entry[\"hadm_id\"], \"charttime\": entry[\"charttime\"], \"row_id\": entry[\"row_id\"]}\n",
    "#     matches = pattern.findall(entry[\"text\"])\n",
    "#     sdhs = {label.strip(): value for label, value in matches}\n",
    "#     base.update(sdhs)\n",
    "#     records.append(base)\n",
    "    \n",
    "# df = pd.DataFrame(records)\n",
    "# df = df.map(lambda x: x.lower() if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc09c321-ea85-40b4-bdb5-c8330096a3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdh_fields = ['Housing issues',\n",
    "              'Employment status', \n",
    "              'Transportation issues', \n",
    "              'Parental status',\n",
    "              'Relationship status', \n",
    "              'Social support']\n",
    "all_map = {}\n",
    "for field in sdh_fields:\n",
    "    all_map[field] = {i: i for i in df[f'{field}'].unique()}\n",
    "all_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6057b6be-80f9-4691-b74e-5b521ca9447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_sdoh = {\n",
    "            # [financial status, undomiciled, other, unknown]\n",
    "            'Housing issues': {'unknown': 'unknown',\n",
    "              'financial status': 'financial status',\n",
    "              'other': 'other',\n",
    "              'subsidized': 'financial status'},\n",
    "            # [employed, unemployed, underemployed, disability, retired, student, unknown]\n",
    "            'Employment status': {'unknown': 'unknown',\n",
    "              'disability': 'disability',\n",
    "              'retired': 'retired',\n",
    "              'unemployed': 'unemployed',\n",
    "              'deferred': 'unemployed',\n",
    "              'employed': 'employed',\n",
    "              'student': 'student',\n",
    "              'employed,disabled': 'underemployed'},\n",
    "            # [distance, resources, other, unknown]\n",
    "            'Transportation issues': {'unknown': 'unknown',\n",
    "              'resources': 'resources',\n",
    "              'other': 'other'},\n",
    "            'Parental status': {'unknown': 'unknown', 'yes': 'yes', 'no': 'no'},\n",
    "            # [married, partnered, widowed, divorced, single, unknown]\n",
    "            'Relationship status': {'unknown': 'unknown',\n",
    "              'married': 'married',\n",
    "              'divorced': 'divorced',\n",
    "              'partnered': 'partnered',\n",
    "              'estranged': 'unknown',\n",
    "              'widowed': 'widowed',\n",
    "              'family': 'unknown',\n",
    "              'brother': 'unknown',\n",
    "              'separated': 'divorced',\n",
    "              'daughter': 'unknown'},\n",
    "            'Social support': {'unknown': 'unknown',\n",
    "              'presence': 'plus',\n",
    "              'absence': 'minus'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c7d274-0410-415b-b921-88ba9c299e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for field in sdh_fields:\n",
    "    df[f'{field}'] = df[f'{field}'].map(map_sdoh[f'{field}'])\n",
    "df = df.map(lambda x: x.replace(\"'\", \"\") if isinstance(x, str) else x)\n",
    "df_pred = df.rename(columns={'Employment status': 'EMPLOYMENT', 'Housing issues': 'HOUSING', 'Transportation issues': 'TRANSPORTATION', 'Parental status': 'PARENT', 'Relationship status': 'RELATIONSHIP', 'Social support': 'SUPPORT'})\n",
    "subject_and_hadm_ids = pd.read_csv('C:/Users/salazarda/Downloads/SDOH_MIMICIII_physio_release.csv')\n",
    "df = subject_and_hadm_ids.iloc[:,5:].copy()\n",
    "\n",
    "prefixes = set(c.split(\"_\",1)[0] for c in df.columns if \"_\" in c)\n",
    "\n",
    "for p in prefixes:\n",
    "    df[p] = ul.collapse_onehot_group(df, p)\n",
    "df_real = df.drop(columns=[c for c in df.columns if \"_\" in c])\n",
    "\n",
    "df_real_ = pd.concat([subject_and_hadm_ids.iloc[:,0:5], df_real], axis=1)\n",
    "df_real_ = df_real_.loc[~(df_real_.iloc[:,5:] == 0).all(axis=1)]\n",
    "df_real_pred = pd.merge(df_real_, df_pred, left_on=['patient_id', 'note_id'], right_on=['subject_id', 'row_id'], how='inner')\n",
    "df_real_pred.loc[:,['patient_id', 'note_id', 'PARENT_x', 'PARENT_y']].drop_duplicates()\n",
    "\n",
    "sdh_fields = [\n",
    "    \"EMPLOYMENT\",\n",
    "    \"HOUSING\",\n",
    "    \"TRANSPORTATION\",\n",
    "    \"PARENT\",\n",
    "    \"RELATIONSHIP\",\n",
    "    \"SUPPORT\"\n",
    "]\n",
    "\n",
    "for field in sdh_fields:\n",
    "    df_ = df_real_pred.loc[:,['patient_id', 'note_id', f'{field}_x', f'{field}_y']].drop_duplicates()\n",
    "    df_ = df_.sort_values(['patient_id', 'note_id', f'{field}_x'],ascending=False).groupby(['patient_id', 'note_id'], as_index=False).first()\n",
    "    if field == 'PARENT':\n",
    "        df_ = df_.map(lambda x: 'no' if x == 0 else 'yes')\n",
    "    else:\n",
    "        df_ = df_.map(lambda x: 'unknown' if x == 0 else x)\n",
    "    y_true = df_[f\"{field}_x\"]\n",
    "    y_pred = df_[f\"{field}_y\"]\n",
    "    # print(f'accuracy for {field}: {accuracy_score(y_true, y_pred)}')\n",
    "    print(f'{field}... ')\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print('_____________________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3799f10d-e274-446c-9065-94279939087f",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a213404-6075-4f85-9553-349df21e5de9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# OLD CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f4fb86-5756-446b-a97f-19a010b4592e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47329423-f1c4-4910-8b3a-c254008311b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "full_prompt = ul.sdh_prompt(notes[0][3])\n",
    "\n",
    "inputs = tokenizer(full_prompt, return_tensors=\"pt\", truncation=True, max_length=5000).to(device)\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=5000,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.2,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045f8fc8-dd07-4812-9d60-66c9e99c398b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(tok.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a76eedc-46f3-42ed-973c-340eaf38f236",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_text = tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b7538c-8715-4304-a4b4-d78d8289a62f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf41474-f7fc-4be3-b827-5275f871c768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2e7af2-eae1-42f5-a893-03b591501a42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import utils_llm as ul\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "time_start = datetime.now()\n",
    "n=5\n",
    "all_outputs = []\n",
    "notes = ul.get_notes_for_first_n_notes(n)\n",
    "\n",
    "prompts = []\n",
    "metadata = []\n",
    "\n",
    "for subject_id, hadm_id, charttime, note_text in notes:\n",
    "    prompts.append(ul.sdh_prompt(note_text))\n",
    "    metadata.append({\n",
    "        \"subject_id\": subject_id,\n",
    "        \"hadm_id\": hadm_id,\n",
    "        \"charttime\": charttime.isoformat() if charttime else None\n",
    "    })\n",
    "\n",
    "batch_size = 16\n",
    "final_outputs = []\n",
    "\n",
    "for i in tqdm(range(0, len(prompts), batch_size)):\n",
    "    batch_prompts = prompts[i:i+batch_size]\n",
    "    batch_meta = metadata[i:i+batch_size]\n",
    "    \n",
    "    batch_responses = pipe(batch_prompts, max_new_tokens=400)\n",
    "    \n",
    "    for meta, raw in zip(batch_meta, batch_responses):\n",
    "        parsed = ul.parse_sdh_response(raw[0]['generated_text'])\n",
    "        final_outputs.append({**meta, **parsed})\n",
    "\n",
    "ul.save_to_jsonl(final_outputs, model_id)\n",
    "\n",
    "print(f' ... For {n} patients and {len(notes)} notes, it took {datetime.now() - time_start} ... ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c071fe5b-5a02-445d-947e-0e8179503974",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe(prompts[0], max_new_tokens=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468f3876-8129-4c03-bd72-82300946ace2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cdfed4-3ed9-414b-85eb-ec022298ce43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3590ad-8793-4436-884e-cccbc1139787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9af73f-df7b-4436-ab7d-e1fd65a125be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"I:/BioMistral-7B\")\n",
    "print(config.architectures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36769af-88d0-47fa-95fd-6dad0b5f6e2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"I:/BioMistral-7B\")\n",
    "\n",
    "prompt = \"What are the latest treatments for glioblastoma?\"\n",
    "output = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "\n",
    "print(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb57ce7-6540-40a3-8c8b-c537c4a48526",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline('text-generation', model=model, tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a79b50-d051-4adb-af68-b68e4bcddf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Explain step-by-step how mRNA vaccines work.\"\n",
    "pipe(prompts[0], max_new_tokens=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967c62c0-9c80-4e13-a300-ee208457d8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Explain  how are the latest treatments for glioblastoma.\"\n",
    "pipe(prompt, max_new_tokens=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c76bb09-71d6-4c62-be56-a8d00150e9df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567dd0dc-1eaa-44eb-8086-fd06f30029f9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "import torch\n",
    "\n",
    "name = \"I:/BioMistral-7B\"\n",
    "tok  = AutoTokenizer.from_pretrained(name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "msgs = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful biomedical assistant.\"},\n",
    "    {\"role\": \"user\",   \"content\": \"What are the latest treatments for glioblastoma?\"}\n",
    "]\n",
    "\n",
    "prompt = tok.apply_chat_template(msgs, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "streamer = TextStreamer(tok)          # streams tokens as they appear (optional)\n",
    "\n",
    "out = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=400,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    streamer=streamer                 # real-time printout\n",
    ")\n",
    "\n",
    "print(tok.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e15c560-4f30-498e-8848-ca20557532d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c42a90-6fdd-46fe-b2f3-71c982782886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de0ddda-586a-45bd-a3d1-459463dcc85d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbc935d-4d56-4ce4-9c1f-02025f9301cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ul.sdh_prompt(\"The patient is a 45-year-old female with no fixed address and a history of alcohol use disorder...\")  # Short sample\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d8d7b5-ba50-4734-94ce-879985216976",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(prompt, max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40cd898-e72b-4615-a6b4-ed83fb7b2578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_metadata_log(final_outputs):\n",
    "    yes_count = 0\n",
    "    no_count = 0\n",
    "    charttimes = []\n",
    "    unique_subjects = set()\n",
    "    \n",
    "    for record in final_outputs:\n",
    "        unique_subjects.add(record['subject_id'])\n",
    "        if record.get('charttime'):\n",
    "            charttimes.append(record['charttime'])\n",
    "        \n",
    "        for var in SDOH_VARIABLES:\n",
    "            if var in record:\n",
    "                if record[var]['present'] == \"Yes\":\n",
    "                    yes_count += 1\n",
    "                elif record[var]['present'] == \"No\":\n",
    "                    no_count += 1\n",
    "    \n",
    "    charttimes_sorted = sorted(charttimes)\n",
    "    \n",
    "    log = {\n",
    "        \"processing_time\": datetime.now().isoformat(),\n",
    "        \"num_patients\": len(unique_subjects),\n",
    "        \"num_notes\": len(final_outputs),\n",
    "        \"variables_used\": SDOH_VARIABLES,\n",
    "        \"num_yes\": yes_count,\n",
    "        \"num_no\": no_count,\n",
    "        \"charttime_range\": {\n",
    "            \"min\": charttimes_sorted[0] if charttimes_sorted else None,\n",
    "            \"max\": charttimes_sorted[-1] if charttimes_sorted else None\n",
    "        }\n",
    "    }\n",
    "    return log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fc271b-c48c-47ec-986e-5658cd944ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_log = build_metadata_log(final_outputs)\n",
    "\n",
    "with open(\"sdoh_processing_log.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metadata_log, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e4b8ec-c6cc-43f4-8647-ba5167ae364b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3690eaac-3b10-43f9-bbd1-141b3c47cfe2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8130a5-ecf9-466a-ac46-835e1a708c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "ul.save_to_jsonl(final_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108e0541-757c-42ea-a52c-09be9471adbf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyze_sdh_for_subject(subject_id=15005348)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f53e94-deaf-4361-bac1-fb8fca8938a1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import utils_llm as ul\n",
    "ul.get_clinical_note(10000032)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d9f5b8-3d34-4225-a2e1-9be0b29d7781",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def sdh_prompt(note_text):\n",
    "    return f\"\"\"\n",
    "    You are a clinical NLP assistant. Analyze the following clinical note and indicate whether each of the following seven social determinants of health (SDH) is specifically mentioned:\n",
    "        \n",
    "    1. **Employment status**: Whether the patient is currently employed, unemployed, retired, on disability, or has a job title or income source.\n",
    "    2. **Housing issues**: Any mention of homelessness, unstable housing, living in shelters, or housing concerns (e.g., can't afford rent, frequent moves).\n",
    "    3. **Transportation needs**: Any reference to transportation difficulties, lack of car access, reliance on public transit, missed appointments due to transportation.\n",
    "    4. **Parental status**: Whether the patient has children or dependents, or is a caregiver to minors.\n",
    "    5. **Relationship status**: Whether the patient is married, divorced, single, has a partner, or is widowed.\n",
    "    6. **Social support**: Whether the patient is receiving formal help or assistance from a **social worker**, also extracts the name of the service.\n",
    "    7. **Substance Use**: Any mention of alcohol, drug, or tobacco use, including current use, past use, or explicit denial of use.\n",
    "    \n",
    "    Answer with **\"Yes\" or \"No\"** for each item, and include a **short evidence sentence**. If not mentioned, say: *There is no evidence.* \n",
    "    \n",
    "    ---\n",
    "    \n",
    "    Now analyze the following clinical note:\n",
    "    \n",
    "    \\\"\\\"\\\"\n",
    "    {note_text}\n",
    "    \\\"\\\"\\\"\n",
    "    \n",
    "    Respond in this format:\n",
    "    \n",
    "    Employment status: [Yes/No] - [short evidence sentence]\n",
    "    Housing issues: [Yes/No] - [short evidence sentence]\n",
    "    Transportation needs: [Yes/No] - [short evidence sentence]\n",
    "    Parental status: [Yes/No] - [short evidence sentence]\n",
    "    Relationship status: [Yes/No] - [short evidence sentence]\n",
    "    Social support: [Yes/No] - [short evidence sentence]\n",
    "    Substance Use: [Yes/No] - [short evidence sentence]\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aef07e8-a935-4704-8a67-b2f4e868b57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ul.sdh_prompt(ul.get_clinical_note(10000032))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b73ccb-0217-42ed-be80-1a9234d9a716",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efddf6e-e87e-49d1-9a2a-a4ae437efed6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe(prompt, max_new_tokens=400)[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b861960-6e6d-4e55-8f81-1d0bbca6b938",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c2023e-1d68-4bd9-977c-1bb4c3b8b01d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8f1925-c734-47c0-9331-1a8c8a3845f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
