{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec86b237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c83f1a9745604f25a50912c985916779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "base_model = \"I:/Llama-2-13b-hf\"     # Llama-2-13b-hf\n",
    "adapter_id = \"I:/LlamaCare\"          # ruta del LoRA\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # si GPU no soporta bfloat16, usar torch.float16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, adapter_id)\n",
    "# model.eval()\n",
    "\n",
    "# Asegurar pad_token para Llama:\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a17bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_llm as ul\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "subject_and_hadm_ids = pd.read_csv('C:/Users/salazarda/Downloads/SDOH_MIMICIII_physio_release.csv')\n",
    "subject_and_hadm_ids = list(subject_and_hadm_ids.loc[:, ['patient_id', 'note_id']].drop_duplicates().itertuples(index=False, name=None))\n",
    "\n",
    "notes = ul.get_clinical_notes_mimic3(subject_and_hadm_ids)\n",
    "notes = random.sample(notes, 2)\n",
    "\n",
    "input_text = notes[0][4] #\"La paciente reporta mejoría moderada con efectos secundarios leves.\"\n",
    "instruction = f\"\"\"Step-bystep: \n",
    "1. Analyse the text, \n",
    "2. Select whether the Employment status is explicitly stated, and \n",
    "3. Select one of the following labels [employed, unemployed, underemployed, disability, retired, student, unknown].\"\"\"\n",
    "prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n\"\n",
    "out = model.generate(**tokenizer(prompt, return_tensors=\"pt\").to(model.device),\n",
    "                     max_new_tokens=200, temperature=0.8)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2721b0fb",
   "metadata": {},
   "source": [
    "# OLD CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cb12fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "base_model = \"I:/Llama-2-13b-hf\"          # o Llama-2-7b-chat-hf\n",
    "adapter_id = \"I:/LlamaCare\"             # LoRA de LlamaCare\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                bnb_4bit_quant_type=\"nf4\",\n",
    "                                bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(base_model, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model,\n",
    "                                             quantization_config=bnb_config,\n",
    "                                             device_map=\"auto\")\n",
    "model = PeftModel.from_pretrained(model, adapter_id)  # aplica el LoRA\n",
    "\n",
    "def chat(prompt, max_new_tokens=256):\n",
    "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    out = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    return tok.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "print(chat(\"Eres un asistente médico. Explica el manejo inicial de DM2 en una simple frase.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b69137",
   "metadata": {},
   "source": [
    "#### Tokenizer and chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d61bbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou are a expert NLP on health datasets.\u001b[39m\u001b[38;5;124m'\u001b[39m}, \n\u001b[0;32m      3\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexplain what is diabetes type 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m      4\u001b[0m ]\n\u001b[1;32m----> 6\u001b[0m prompt \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(\n\u001b[0;32m      7\u001b[0m     messages, \n\u001b[0;32m      8\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m      9\u001b[0m     add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     12\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     13\u001b[0m out \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\salazarda\\AppData\\Local\\anaconda3\\envs\\pytor\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1621\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[1;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m   1618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1619\u001b[0m     tokenizer_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1621\u001b[0m chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_chat_template(chat_template, tools)\n\u001b[0;32m   1623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_assistant_tokens_mask \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m-?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*generation\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*-?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m, chat_template):\n\u001b[0;32m   1624\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[0;32m   1625\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_assistant_tokens_mask==True but chat template does not contain `\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;132;01m% g\u001b[39;00m\u001b[38;5;124meneration \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m}` keyword.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1626\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\salazarda\\AppData\\Local\\anaconda3\\envs\\pytor\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1789\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.get_chat_template\u001b[1;34m(self, chat_template, tools)\u001b[0m\n\u001b[0;32m   1787\u001b[0m         chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_template\n\u001b[0;32m   1788\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1789\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1790\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use chat template functions because tokenizer.chat_template is not set and no template \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1791\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument was passed! For information about writing templates and setting the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1792\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer.chat_template attribute, please see the documentation at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1793\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/chat_templating\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1794\u001b[0m         )\n\u001b[0;32m   1796\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chat_template\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": 'system', \"content\": 'You are a expert NLP on health datasets.'}, \n",
    "    {\"role\": 'user', \"content\": \"explain what is diabetes type 2.\"}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenizer=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "out = model.generate(**inputs, max_new_tokens=400, temperature=0, top_p=1)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c838a9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <<SYS>>\n",
      "Your are a NLP expert assistant.\n",
      "<</SYS>>\n",
      "Analyse the text and determine whether the Employment status is explicitly stated. The text: SW consult received via page to address family coping issues.\n",
      "   SW met w/pt\n",
      "s mother in the pt\n",
      "s room. At first pt\n",
      "s mother asked if we\n",
      "   needed pt\n",
      "s organs for donations. Subsequently, SW said that perhaps it\n",
      "   would be helpful to discuss how she and the rest of the family are\n",
      "   coping and the nursing staff will be addressing donation issues. Pt\n",
      "   mother was open for a discussion.\n",
      "   Pt\n",
      "s mother appeared sad and mildly anxious. She said that has gone\n",
      "   through a lot in the recent past and is not sure how to cope w/her\n",
      "   daughter\n",
      "s fatal illness. She also expressed a concern about her\n",
      "   family. Further, she stated that her daughter has only a few days left.\n",
      "   SW asked if this was the doctor\n",
      "[**Initials (NamePattern4) **] [**Last Name (NamePattern4) 1517**]. She replied,\n",
      "more or\n",
      "   less\n",
      ". SW encouraged the pt\n",
      "s mother to discuss medical\n",
      "   concerns/questions w/the doctor to have a complete understanding of the\n",
      "   pt\n",
      "s condition. She informed that is very happy w/the medical staff\n",
      "   here at [**Hospital1 1**] and is in contact w/the nurses and doctors at [**Name5 (PTitle) **] [**Name5 (PTitle) 1518**].\n",
      "   Pt\n",
      "s mother informed that the family has had many losses in the recent\n",
      "   past. She herself had a cancer 2 yrs ago, her sister past away\n",
      "   beginning of this year plus other extended family deaths. The most\n",
      "   recent loss was pt\n",
      "s brother-in-law, who committed suicide on [**9-20**], [**2112**].\n",
      "   Pt has 3 sisters: [**Name (NI) 1519**], [**Name (NI) **], and [**Name (NI) 1520**]. Pt has been very close\n",
      "   to [**Doctor Last Name 1520**]\n",
      "s 2 children (6 y/o, and 8^th y/o autistic boy). The pt has\n",
      "   been there for her sister and the 2 boys as they recently lost their\n",
      "   father. Pt\n",
      "s mother informed that her other daughter [**Name (NI) 1520**] discovered\n",
      "   her brother-in-law a week later in her garage. Pt\n",
      "s mother is extremely\n",
      "   concerned about [**First Name7 (NamePattern1) 1520**]\n",
      "[**Initials (NamePattern4) **] [**Last Name (NamePattern4) **] health, considering that she\n",
      "ll be\n",
      "   loosing her sister on top of the horrific discovery. Also, she wasn\n",
      "   sure if it would be beneficial for [**Doctor Last Name 1520**] to see her sister in this\n",
      "   condition. SW provided support and informed that the family should\n",
      "   provide her w/options and not pressure her to come to the hospital.\n",
      "   Pt\n",
      "s mother was also questioning if [**Name (NI) 1520**]\n",
      "s 16 y/o daughter [**Name (NI) 1521**]\n",
      "   should come to the hospital, as she was very close to the pt. SW\n",
      "   informed that this is a family decision and if they feel that [**Doctor First Name 1521**] is\n",
      "   old enough they should provide her w/the opportunity to say goodbye to\n",
      "   her favorite aunt.\n",
      "   SW asked if they are able to process their feelings about the recent\n",
      "   events. Pt\n",
      "s mother replied,\n",
      "Not really\n",
      ". She further explained how\n",
      "   they are very close knit family yet are never emotional or able to talk\n",
      "   to each other about their feelings. She acknowledged the importance of\n",
      "   processing grief. Everyone in the family seems to deal w/emotional\n",
      "   issues on their own. SW explained that there\n",
      "s no right way of\n",
      "   processing emotions yet it\n",
      "s very important to do so one way or the\n",
      "   other. She said that knows many wonderful therapists and would contact\n",
      "   one to receive help. She said that family therapy would not be an\n",
      "   option. SW encouraged the pt\n",
      "s mother to inform her other daughters of\n",
      "   the importance of grief counseling.\n",
      "   She was happy to talk to a SW and said that if things get worse they\n",
      "   would contact the SW dept for further support.\n",
      "   [**Doctor First Name 1515**] Rohila, LCSW\n",
      "   #[**Numeric Identifier 1522**]\n",
      " [/INST]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llama2_tpl = \"\"\"{% for m in messages -%}\n",
    "{% if m['role'] == 'system' -%}\n",
    "<s>[INST] <<SYS>>\n",
    "{{ m['content'] }}\n",
    "<</SYS>>\n",
    "{% elif m['role'] == 'user' -%}\n",
    "{{ m['content'] }} [/INST]\n",
    "{% elif m['role'] == 'assistant' -%}\n",
    "{{ m['content'] }}</s>\n",
    "{% endif -%}\n",
    "{% endfor -%}\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "  {\"role\":\"system\",\"content\":\"Your are a NLP expert assistant.\"},\n",
    "  {\"role\":\"user\",\"content\":f\"Analyse the text and determine whether the Employment status is explicitly stated. The text: {input_text}\"}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, chat_template=llama2_tpl,\n",
    "                                 tokenize=False, add_generation_prompt=True)\n",
    "out = model.generate(**tokenizer(prompt, return_tensors=\"pt\").to(model.device),\n",
    "                     max_new_tokens=256, temperature=0.9, top_p=0.9,\n",
    "                     eos_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3f6824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6156d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Step-bystep: \n",
      "1. Analyse the text, \n",
      "2. Select whether the Employment status is explicitly stated, and \n",
      "3. Select one of the following labels [employed, unemployed, underemployed, disability, retired, student, unknown].\n",
      "\n",
      "### Input:\n",
      "Social Work\n",
      "   SW following family throughout week for emotional support and resource\n",
      "   assistance.  Have met with family daily who report to be and appear to\n",
      "   be coping well given circumstances of pt\n",
      "s condition and unknown\n",
      "   prognosis compounded with their distance from their home.  SW arranged\n",
      "   for family to stay in [**Hospital1 1**] apts in the Galleria.  Family can remain\n",
      "   there as long as apts are available.  Family and pt seem to have good\n",
      "   support network, but note they have not yet enlisted the help of others\n",
      "   and are waiting to do so when they need to return to the west coast.\n",
      "   Pt\n",
      "s son returned to CA on [**9-21**], pt\n",
      "s dtr and husband will return to\n",
      "   [**Name (NI) 981**], [**Name (NI) 951**] tomorrow.  Pt\n",
      "s sister [**Name (NI) 919**] and pt\n",
      "s mother (both of\n",
      "   whom reside in [**Name (NI) 122**]) will remain here to be with pt.  [**Doctor First Name 919**] will be\n",
      "   family contact.  Family reports that pt and family are Episcopal and\n",
      "   are finding support both by [**Hospital1 1**] chaplain as well as their church\n",
      "   community back home.\n",
      "   SW provided emotional support and empathic listening.  Also provided\n",
      "   family with info re: local Episcopal churches as well as info re: Care\n",
      "   Pages so family can create web page to keep family and friends informed\n",
      "   of pt\n",
      "s condition and needs of the family.\n",
      "   Family has sw contact info and is aware that sw will continue to follow\n",
      "   for support and resource assistance.\n",
      "   [**First Name8 (NamePattern2) 299**] [**Last Name (NamePattern1) 300**], LICSW #[**Numeric Identifier 304**]\n",
      "\n",
      "\n",
      "### Response:\n",
      "employed\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d0d9b53a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64407,\n",
       " 175035,\n",
       " 619590,\n",
       " datetime.datetime(2169, 12, 29, 9, 43),\n",
       " 'Chief Complaint:\\n   I saw and examined the patient, and was physically present with the ICU\\n   Resident for key portions of the services provided.  I agree with his /\\n   her note above, including assessment and plan.\\n   HPI:\\n   59 yo ESRD on HD, dilated CM, admitted w/ CP and dyspnea after several\\n   days of missed HD. Probable respiratory arrest from acute CHF, unlikely\\n   PEA but presented with K>8. Active issues include fluid overload/ pulm\\n   edema, presumed aspiration pna, diarrhea, agitation and presumptive\\n   opiate withdrawal.\\n   24 Hour Events:\\n   Intubated yest for agitation. Psych c/s obtained, CT head done, echo\\n   ordered.\\n   Allergies:\\n   No Known Drug Allergies\\n   Last dose of Antibiotics:\\n   Vancomycin - [**2169-12-26**] 10:51 AM\\n   Piperacillin/Tazobactam (Zosyn) - [**2169-12-28**] 08:29 AM\\n   Infusions:\\n   Propofol - 80 mcg/Kg/min\\n   Other ICU medications:\\n   Lorazepam (Ativan) - [**2169-12-28**] 10:00 AM\\n   Other medications:\\n   ASA, RISS, Hep sc, thiamine, folate, Ca acetate, Requip, nicotine\\n   patch, haldol sched, Peridex\\n   Changes to medical and family history:\\n   PMH, SH, FH and ROS are unchanged from Admission except where noted\\n   above and below\\n   Review of systems is unchanged from admission except as noted below\\n   Review of systems:\\n   Flowsheet Data as of  [**2169-12-29**] 09:43 AM\\n   Vital signs\\n   Hemodynamic monitoring\\n   Fluid balance\\n                                                                  24 hours\\n                                                               Since [**71**] AM\\n   Tmax: 37.2\\nC (98.9\\n   Tcurrent: 35.6\\nC (96.1\\n   HR: 136 (68 - 136) bpm\\n   BP: 156/79(99) {90/39(53) - 168/79(99)} mmHg\\n   RR: 13 (13 - 31) insp/min\\n   SpO2: 100%\\n   Heart rhythm: AF (Atrial Fibrillation)\\n             Total In:\\n                                                                    998 mL\\n                                                                    356 mL\\n   PO:\\n             TF:\\n   IVF:\\n                                                                    998 mL\\n                                                                    356 mL\\n   Blood products:\\n   Total out:\\n                                                                    130 mL\\n                                                                     10 mL\\n   Urine:\\n                                                                    130 mL\\n                                                                     10 mL\\n   NG:\\n   Stool:\\n   Drains:\\n   Balance:\\n                                                                    868 mL\\n                                                                    346 mL\\n   Respiratory support\\n   O2 Delivery Device: Endotracheal tube\\n   Ventilator mode: PSV/SBT\\n   Vt (Set): 550 (550 - 550) mL\\n   Vt (Spontaneous): 444 (444 - 600) mL\\n   PS : 5 cmH2O\\n   RR (Set): 12\\n   RR (Spontaneous): 18\\n   PEEP: 0 cmH2O\\n   FiO2: 40%\\n   RSBI: 34\\n   PIP: 6 cmH2O\\n   Plateau: 13 cmH2O\\n   SpO2: 100%\\n   ABG: ///21/\\n   Ve: 9.5 L/min\\n   Physical Examination\\n   General Appearance: No acute distress\\n   Eyes / Conjunctiva: PERRL\\n   Cardiovascular: (S1: Normal), (S2: Normal)\\n   Peripheral Vascular: (Right radial pulse: Not assessed), (Left radial\\n   pulse: Not assessed), (Right DP pulse: Not assessed), (Left DP pulse:\\n   Not assessed)\\n   Respiratory / Chest: (Breath Sounds: No(t) Clear : , Rhonchorous: )\\n   Abdominal: Soft, Non-tender\\n   Extremities: Right lower extremity edema: Absent, Left lower extremity\\n   edema: Absent\\n   Skin:  Not assessed\\n   Neurologic: Responds to: Not assessed, Movement: Not assessed, Tone:\\n   Not assessed, agitated\\n   Labs / Radiology\\n   10.3 g/dL\\n   220 K/uL\\n   70 mg/dL\\n   8.9 mg/dL\\n   21 mEq/L\\n   4.0 mEq/L\\n   40 mg/dL\\n   101 mEq/L\\n   147 mEq/L\\n   30.7 %\\n   11.3 K/uL\\n        [image002.jpg]\\n                             [**2169-12-26**]  10:29 AM\\n                             [**2169-12-26**]  10:50 AM\\n                             [**2169-12-26**]  12:57 PM\\n                             [**2169-12-26**]  03:49 PM\\n                             [**2169-12-27**]  02:41 AM\\n                             [**2169-12-27**]  04:45 AM\\n                             [**2169-12-27**]  07:42 AM\\n                             [**2169-12-27**]  07:49 AM\\n                             [**2169-12-28**]  03:48 AM\\n                             [**2169-12-29**]  02:09 AM\\n   WBC\\n   14.3\\n   14.1\\n   11.3\\n   Hct\\n   32.5\\n   31.4\\n   30.7\\n   Plt\\n   184\\n   175\\n   220\\n   Cr\\n   6.3\\n   6.5\\n   8.0\\n   8.5\\n   6.3\\n   8.9\\n   TropT\\n   0.11\\n   TCO2\\n   35\\n   32\\n   29\\n   Glucose\\n   107\\n   96\\n   83\\n   99\\n   86\\n   96\\n   70\\n   Other labs: PT / PTT / INR:11.8/23.9/1.0, CK / CKMB /\\n   Troponin-T:139/5/0.11, ALT / AST:30/29, Alk Phos / T Bili:73/0.4,\\n   Differential-Neuts:67.0 %, Band:0.0 %, Lymph:20.0 %, Mono:4.0 %,\\n   Eos:3.0 %, Lactic Acid:1.6 mmol/L, Albumin:3.4 g/dL, LDH:307 IU/L,\\n   Ca++:8.9 mg/dL, Mg++:2.5 mg/dL, PO4:6.7 mg/dL\\n   Imaging: Echo- [**Name Prefix (Prefixes) **] [**Last Name (Prefixes) 4139**], EF 70-80%\\n   Microbiology: Blood x2 pending\\n   Sputum- resp flora\\n   C diff- neg\\n   Assessment and Plan\\n   59 yo ESRD on HD, dilated CM, admitted w/ CP and dyspnea after several\\n   days of missed HD. Probable respiratory arrest from acute CHF, unlikely\\n   PEA but presented with K>8. Active issues include fluid overload/ pulm\\n   edema, presumed aspiration pna, diarrhea, and agitated delirium.\\n   (1) AGITATION-DDx includes encephalitis, toxic/metabolic, med effect\\n   (Requip, high dose benzos)\\n   -use Haldol monotherapy; stop Ativan and follow QTc\\n   -plan on LP and empiric acyclovir tx\\n   -poss MRI pending LP results\\n   -cont intubation for airway protection\\n   -check TG levels given high-dose propofol\\n   (2) POSSIBLE PNA- completing empiric 8d course vanco + Zosyn\\n   (3) AFIB- predisposed w/ dilated CM; current agitation\\n   -control agitation; switch to diltiazem for better control\\n   -may need anticoag; deferring for now\\n   (4) ESRD- back on HD\\n   -monitor lytes\\n   (5) HTN- BB uptitrated today\\n   ICU Care\\n   Nutrition:\\n   Comments: start t.f.\\n   Glycemic Control:\\n   Lines:\\n   Multi Lumen - [**2169-12-28**] 06:45 PM\\n   Prophylaxis:\\n   DVT: SQ UF Heparin\\n   Stress ulcer: H2 blocker\\n   VAP: HOB elevation, Mouth care, Daily wake up, RSBI\\n   Comments:\\n   Communication: Family meeting held  Comments:\\n   Code status: Full code\\n   Disposition :ICU\\n   Total time spent: 30 minutes\\n')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_and_hadm_ids = pd.read_csv('C:/Users/salazarda/Downloads/SDOH_MIMICIII_physio_release.csv')\n",
    "subject_and_hadm_ids = list(subject_and_hadm_ids.loc[:, ['patient_id', 'note_id']].drop_duplicates().itertuples(index=False, name=None))\n",
    "\n",
    "notes = ul.get_clinical_notes_mimic3(subject_and_hadm_ids)\n",
    "notes = random.sample(notes, 20)\n",
    "notes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fac233c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0a0c19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f7d059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "base_model = \"I:/Llama-2-13b-hf\"     # o Llama-2-7b(-chat)-hf\n",
    "adapter_id = \"I:/LlamaCare\"          # ruta del LoRA\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # si GPU no soporta bfloat16, usa torch.float16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, adapter_id)\n",
    "model.eval()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80e4160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_llm as ul\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "subject_and_hadm_ids = pd.read_csv('C:/Users/salazarda/Downloads/SDOH_MIMICIII_physio_release.csv')\n",
    "subject_and_hadm_ids = list(subject_and_hadm_ids.loc[:, ['patient_id', 'note_id']].drop_duplicates().itertuples(index=False, name=None))\n",
    "\n",
    "notes = ul.get_clinical_notes_mimic3(subject_and_hadm_ids)\n",
    "notes = random.sample(notes, 2)\n",
    "# notes = notes[0:50]\n",
    "\n",
    "sdoh_output = []\n",
    "\n",
    "for note in notes:\n",
    "    meta = {\n",
    "        'subject_id': note[0],\n",
    "        'hadm_id': note[1],\n",
    "        'row_id': note[2],\n",
    "        'charttime': note[3].isoformat() if note[3] else None\n",
    "    }\n",
    "\n",
    "    outputs_per_note = meta.copy()  # Start with metadata\n",
    "\n",
    "    for sdoh in tqdm(['Employment status', 'Housing issues', 'Transportation issues', 'Parental status', 'Relationship status', 'Social support']):\n",
    "        instruction = {\n",
    "            'Employment status': 'Employment status: Whether the patient is currently employed, unemployed, underemployed, disability, retired, student, or unknown. LABELS: [employed, unemployed, underemployed, disability, retired, student, unknown]',\n",
    "            'Housing issues': 'Housing issues: Any mention of financial status, undomiciled, other. LABELS: [financial status, undomiciled, other, unknown]', \n",
    "            'Transportation issues': 'Transportation issues: Any reference to transportation difficulties such as distance, resources, other. LABELS: [distance, resources, other, unknown]', \n",
    "            'Parental status':'Parental status: Whether the patient has a child under 18 years old. LABELS: [yes, no, unknown]',\n",
    "            'Relationship status': 'Relationship status: Whether the patient is widowed, divorced, single. LABELS: [married, partnered, widowed, divorced, single, unknown]',\n",
    "            'Social support': 'Social support: It does include informal or emotional support from family members, friends, or romantic partners unless such support is clearly mediated through a formal care plan by a social worker or case manager. LABELS: [presence, absence, unknown]'\n",
    "        }\n",
    "\n",
    "        prompt = ul.sdh_single_prompt(note[4], sdoh, instruction[sdoh])\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=600,\n",
    "                length_penalty=1.6,\n",
    "                num_beams=10,\n",
    "                no_repeat_ngram_size=3,\n",
    "                temperature=0.8,\n",
    "                do_sample=True,\n",
    "                top_k=15,\n",
    "                top_p=0.95,\n",
    "                repetition_penalty=2.1,\n",
    "                early_stopping=True,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        output_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        outputs_per_note[sdoh] = output_text\n",
    "\n",
    "    sdoh_output.append(outputs_per_note)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3da2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdoh_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6942f822",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ul.sdh_single_prompt(note[4], sdoh, instruction[sdoh])\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=600,\n",
    "        length_penalty=1.6,\n",
    "        num_beams=10,\n",
    "        no_repeat_ngram_size=3,\n",
    "        temperature=1,\n",
    "        do_sample=True,\n",
    "        top_k=15,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=2.1,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "output_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28721a99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5573f26",
   "metadata": {},
   "source": [
    "#### Strategy prompt # 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1039a455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac06e0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "note_text = note[4]\n",
    "sdoh = 'Employment status'\n",
    "sdoh_def = 'whether the patient is currently employed, unemployed, underemployed, disability, retired, student, or unknown. LABELS: [employed, unemployed, underemployed, disability, retired, student, unknown]'\n",
    "LIST_OF_POSSIBLE_VALUES = ['employed', 'unemployed', 'underemployed', 'disability', 'retired', 'student', 'unknown']\n",
    "prompt = f\"\"\"You are extracting a specific Social Determinants of Health (SDOH) variable from a clinical note.  \n",
    "Here is the variable definition:  \n",
    "{sdoh_def}\n",
    "\n",
    "Step 1. Search the text for information relevant to the SDOH variable as defined above. [Knowledge]  \n",
    "\n",
    "Step 2. Summarize this information in one or two sentences that are sufficient to determine the value of the SDOH variable. [Summary]  \n",
    "\n",
    "Step 3. Choose **only** one value for the SDOH variable from the following list: {LIST_OF_POSSIBLE_VALUES}.  \n",
    "Respond with the exact value only, no explanation or additional text. [Answer]  \n",
    "\n",
    "Clinical note: \\\"\\\"\\\"{note_text}\\\"\\\"\\\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749a78c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(base_model, use_fast=False)\n",
    "system = \"You are a careful medical NLP assistant.\"\n",
    "user = f\"\"\"You are extracting an SDOH variable.\n",
    "\n",
    "Definition:\n",
    "Employment status refers to whether the patient is currently employed, unemployed, retired, a student, or not working due to disability or other reasons.\n",
    "\n",
    "Step 1. Search the text for information relevant to the SDOH variable. [Knowledge]\n",
    "Step 2. Summarize this information in one or two sentences. [Summary]\n",
    "Step 3. Choose only one value from [employed, unemployed, retired, student, not working due to disability, unknown].\n",
    "Respond with the exact value only. [Answer]\n",
    "\n",
    "Clinical note: \\\"\\\"\\\"{note_text}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "prompt = f\"<s>[INST] <<SYS>>{system}<</SYS>>\\n\\n{user} [/INST]\"\n",
    "prompt = tok(prompt, return_tensors=\"pt\").to(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ba8731",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = model.generate(\n",
    "    **prompt,\n",
    "    max_new_tokens=4,\n",
    "    do_sample=False,\n",
    "    temperature=0.0,\n",
    "    top_p=1.0,\n",
    "    repetition_penalty=1.1,\n",
    "    eos_token_id=tok.eos_token_id,\n",
    "    pad_token_id=tok.eos_token_id,\n",
    ")\n",
    "out = tok.decode(gen[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cf4cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3671ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=5000,\n",
    "        length_penalty=1.6,\n",
    "        num_beams=10,\n",
    "        no_repeat_ngram_size=3,\n",
    "        temperature=0.0,\n",
    "        do_sample=True,\n",
    "        top_k=15,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=2.1,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "text = text.replace(prompt, \"\").strip()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351a4edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "note[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4ff2ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10532a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
